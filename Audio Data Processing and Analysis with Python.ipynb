{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c134850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# load the NSynth dataset\n",
    "dataset, info = tfds.load('nsynth', split='train', with_info=True)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1ac8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the keys of one sample\n",
    "for sample in dataset.take(1):\n",
    "    print(\"Available keys:\")\n",
    "    for key in sample.keys():\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e0f5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "audio_np = audio.numpy()\n",
    "Audio(audio_np, rate=16000)  # Assuming a sample rate of 16kHz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99afebcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    y=audio_np,\n",
    "    mode='lines',\n",
    "    line=dict(color='black'),\n",
    "    name=\"Waveform\"\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Waveform\",\n",
    "    xaxis_title=\"Time (samples)\",\n",
    "    yaxis_title=\"Amplitude\",\n",
    "    template=\"plotly_white\",\n",
    "    width=800,\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f07e2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# compute the STFT\n",
    "spectrogram = librosa.stft(audio_np, n_fft=512, hop_length=256)\n",
    "spectrogram_db = librosa.amplitude_to_db(abs(spectrogram))\n",
    "\n",
    "time = np.linspace(0, len(audio_np) / 16000, spectrogram_db.shape[1])\n",
    "frequencies = np.linspace(0, 16000 / 2, spectrogram_db.shape[0])\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=spectrogram_db,\n",
    "    x=time,\n",
    "    y=frequencies,\n",
    "    colorscale='Viridis',\n",
    "    colorbar=dict(title='Amplitude (dB)'),\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Spectrogram\",\n",
    "    xaxis_title=\"Time (seconds)\",\n",
    "    yaxis_title=\"Frequency (Hz)\",\n",
    "    yaxis=dict(type=\"log\"),\n",
    "    template=\"plotly\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3954e54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# count instrument occurrences\n",
    "instrument_counts = Counter()\n",
    "for sample in dataset.take(1000):\n",
    "    instrument = sample['instrument']['family'].numpy()\n",
    "    instrument_counts[instrument] += 1\n",
    "\n",
    "# map numeric IDs to instrument family names\n",
    "instrument_families = [\"Bass\", \"Brass\", \"Flute\", \"Guitar\", \"Keyboard\", \"Mallet\", \"Organ\", \"Reed\", \"String\", \"Synth Lead\", \"Synth Pad\", \"Vocal\"]\n",
    "mapped_family_counts = {instrument_families[family_id]: count for family_id, count in instrument_counts.items()}\n",
    "\n",
    "import plotly.express as px\n",
    "fig = px.bar(\n",
    "    x=list(mapped_family_counts.keys()),\n",
    "    y=list(mapped_family_counts.values()),\n",
    "    labels={'x': 'Instrument Family', 'y': 'Count'},\n",
    "    title=\"Distribution of Instrument Families\",\n",
    "    template=\"plotly\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7455863b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spectrogram = librosa.feature.melspectrogram(y=audio_np, sr=16000, n_mels=128)\n",
    "mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=mel_spectrogram_db,\n",
    "    x=time,\n",
    "    y=np.linspace(0, 16000 / 2, mel_spectrogram_db.shape[0]),\n",
    "    colorscale='Viridis',\n",
    "    colorbar=dict(title=\"Amplitude (dB)\")\n",
    "))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b56b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs = librosa.feature.mfcc(y=audio_np, sr=16000, n_mfcc=13)\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=mfccs,\n",
    "    x=time,\n",
    "    y=np.arange(1, mfccs.shape[0] + 1),\n",
    "    colorscale='Viridis',\n",
    "    colorbar=dict(title=\"MFCC Value\")\n",
    "))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322a39f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply pitch shift (+2 semitones)\n",
    "audio_pitch_shifted = librosa.effects.pitch_shift(audio_np, sr=16000, n_steps=2)\n",
    "\n",
    "# apply time-stretching (speed up by 1.5x)\n",
    "audio_time_stretched = librosa.effects.time_stretch(audio_np, rate=1.5)\n",
    "\n",
    "# plot waveforms\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=audio_np, mode='lines', name='Original'))\n",
    "fig.add_trace(go.Scatter(y=audio_pitch_shifted, mode='lines', name='Pitch Shifted'))\n",
    "fig.add_trace(go.Scatter(y=audio_time_stretched, mode='lines', name='Time Stretched'))\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
